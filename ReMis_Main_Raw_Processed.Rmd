---
title: "ReMis Main Raw Processed"
author: "Marton Kovacs"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

# Load packages

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(writexl)
library(readxl)
library(zoo)
library(qdap)
```

# Load functions

The "utils.R" file contains the custom made functions for this project.

```{r}
source("utils.R")
```

# General data cleaning
## Import data

We are importing the raw datafile that was created by the code in the "ReMis_Main_Source_Raw.Rmd" file.

```{r, message = FALSE, warning = FALSE}
raw <- read_tsv("Data/Raw/ReMis_Main_Raw_data.tsv")
```

## Converting the varaible names to clean format

We are using snake format.

```{r}
raw <-
  raw %>% 
  janitor::clean_names()
```

## Data filtering
### Delete rows created by Qualtrics for variable explanation

Qualtrics creates two rows that store metadata about the variables. These are not needed in further analysis.

```{r}
raw <- 
  raw %>%
  slice(-(1:2))
```

The number of responses: `r response_count(raw)`

### Checking the number of responses from each distribution channel

The survey was filled out by respondents outside of our sample as some respondents redistributed the survey link among their colleagues. These responses are marked as "anonymous" by Qualtrics in the Distribution channel variable. Whereas the survey that we sent to our sample is marked by "email".

```{r}
distrib_channel_count <- 
  raw %>%
  group_by(distribution_channel) %>%
  count()

distrib_channel_count
```

We received `r filter(distrib_channel_count, distribution_channel == "anonymous") %>% pull(n)` answers from the redistribution of our survey. We decided to keep these answers for the creation of the groups but and for analysis as the anonymous respondents provided answers for the background questions as well.

### Delete trial responses

There are responses that were provided by the authors during the testing of the survey. These responses are excluded from further analysis.

```{r}
trial_respones <- c("R_2v2lOblGkxW8bzU", "R_OsbjDax8tZ9HrBD", "R_3haoYfua4zboVyb", "R_2xEmpKqvv1joVTT")

raw <- 
  raw %>%
  filter(response_id %ni% trial_respones)
```

The number of responses: `r response_count(raw)`

### Delete respondents who did not accept the informed consent form

We exclude respondents who did not accept the informed consent form in the beggining of our survey from any further analysis.

The number of respondents who did and did not accept the informed consent form:

```{r}
raw %>%
  group_by(inform) %>% 
  summarise(n = n())
```

```{r}
raw <- 
  raw %>%
  filter(inform == "Yes")
```

The number of responses: `r response_count(raw)`

### Delete respondents who did not answer any question

When saving the responses of the survey from Qualtrics we downloaded responses in progress alongside the completed responses. Therefore, we exclude the responses where the respondent did not answer any questions of the survey.

```{r}
questions <- colnames(raw)[12:26]

raw_no_answer <- 
  raw %>% 
  filter_at(vars(questions), all_vars(is.na(.)))
```

The number of responses where the respondent did not answer any question: `r response_count(raw_no_answer)`

```{r}
raw <- 
  raw %>%
  filter_at(vars(questions), any_vars(!is.na(.)))
```

The number of responses: `r response_count(raw)`

## Excluding respondents who did not answer the compulsory background questions

Number of respondents who will be excluded.

```{r}
raw %>% 
  filter_at(vars(years, field), any_vars(is.na(.))) %>% 
  nrow()
```

Save the id of the respondent with missing background responses.

```{r}
missing_background <-
  raw %>% 
  filter_at(vars(years, field), any_vars(is.na(.))) %>% 
  pull(response_id)

missing_background
```

Exclusion.

```{r}
raw <- 
  raw %>%
  filter_at(vars(field, years), any_vars(!is.na(.)))
```

The number of responses: `r response_count(raw)`

## Calucalting median response time

We are calculating the median competition time of the survey based on the responses that we had after exclusion.

```{r}
raw %>% 
  mutate(response_time_sec = as.numeric(duration_in_seconds)) %>% 
  summarise(median_response_time_min = median(response_time_sec) / 60,
            iqr_response_time = stats::IQR(response_time_sec),
            quantile_response_time_3 = quantile(response_time_sec, 0.75),
            quantile_response_time_1 = quantile(response_time_sec, 0.25),
            min_response_time = min(response_time_sec),
            max_response_time = max(response_time_sec))
```

## Variable transformation

The questions about the outcomes of the mistakes were multiple choice questions with the possibility of providing a free text answer (by choosing the "other" option).
We are combining the free text responses with the preset responses into one variable.

```{r}
raw <- 
  raw %>%
  mutate(years = as.integer(years),
         recurring_outcome = case_when(recurring_outcome == "other" ~ recurring_outcome_6_text,
                                          TRUE ~ recurring_outcome),
         serious_outcome = case_when(serious_outcome == "other" ~ serious_outcome_6_text,
                                          TRUE ~ serious_outcome))
```

We are keeping only the level description of the Likert-scale variables but not their explanation.

```{r}
raw <- 
  raw %>%
  mutate(general_frequency = beg2char(general_frequency, "\n"),
         recurring_frequency = beg2char(recurring_frequency, "\n"),
         recurring_serious = beg2char(recurring_serious, "\n"),
         serious_serious = beg2char(serious_serious, "\n"))
```

## Calculate the number of responses after all initial exclusions

There are `r response_count(raw)` responses remaining for further data wrangling. Out of which, `r filter(raw, distribution_channel == "anonymous") %>% nrow()` are coming from anonymous respondents.

## Filter the needed variables only

We are dropping the variables that will not be used in further analysis.

```{r}
raw <- 
  raw %>%
  select(-start_date, -end_date, -status,
         -progress, -duration_in_seconds, -finished,
         -recorded_date, -distribution_channel, -user_language,
         -inform, -comment, -serious_outcome_6_text,
         -recurring_outcome_6_text)
```

## Saving the cleaned dataset

```{r}
write_tsv(raw, "Data/Processed/ReMis_Main_Raw_Cleaned_data.tsv")
```

# Data prepocessing
## Preprocessing of the free-text responses
### Response separation

Some respondents wrote more than one answer to a free text question. In order to not lose any information, the authors read through the free text responses and separated the responses if they contained multiple answers.

Since we asked the respondents to describe their team's most **recurring** and most **serious** mistakes, and their causes and their outcomes we have to treat these differently later on. We keep this information in the variable named "[mistake or cause or outcome]_type".

We keep the original response in the variable named "mistake" and "cause" and "outcome". During the separation this variable will not be touched.

The separated answers will be kept in new variables with and increasing id number in their name.

E.g. If there are two answers in one response in "mistake" then those two answers will be separated in the "mistake_1" and "mistake_2" variables.

#### Number of responses before separation

For counting the response before the separation we do not include the missing responses.

```{r}
n_response_before_separation <- function(recurring_var, serious_var) {
  raw %>% 
    transmute(response_id,
              {{recurring_var}},
              {{serious_var}}) %>% 
    gather(key = "type", value = "value", -response_id) %>% 
    mutate(type = str_extract(type, "[^_]+")) %>% 
    drop_na(value) %>%
    count(type) %>% 
    mutate(N = sum(n))
  }

n_response_before_separation(recurring_story, serious_story)

n_response_before_separation(recurring_cause, serious_cause)

n_response_before_separation(recurring_outcome, serious_outcome)
```

#### Transforming data

```{r}
processed_mistake_separation <-
  raw %>%
  transmute(response_id,
            recurring_story,
            serious_story) %>% 
  gather(key = "mistake_type", value = "mistake", -response_id) %>% 
  mutate(mistake_type = str_extract(mistake_type, "[^_]+"),
         mistake_1 = NA_character_,
         mistake_2 = NA_character_)

processed_cause_separation <-
  raw %>% 
  transmute(response_id,
            recurring_cause,
            serious_cause) %>% 
  gather(key = "cause_type", value = "cause", -response_id) %>% 
  mutate(cause_type = str_extract(cause_type, "[^_]+"),
         cause_1 = NA_character_,
         cause_2 = NA_character_)

processed_outcome_separation <-
  raw %>% 
  transmute(response_id,
            recurring_outcome,
            serious_outcome) %>% 
  gather(key = "outcome_type", value = "outcome", -response_id) %>% 
  mutate(outcome_type = str_extract(outcome_type, "[^_]+"),
         outcome_1 = NA_character_)
```

#### Saving the dataset

```{r}
# write_xlsx(processed_mistake_separation, "Data/Processed/grouping/separation/ReMis_Main_Processed_Mistake_data.xlsx")

# write_xlsx(processed_cause_separation, "Data/Processed/grouping/separation/ReMis_Main_Processed_Cause_data.xlsx")

# write_xlsx(processed_outcome_separation, "Data/Processed/grouping/separation/ReMis_Main_Processed_Outcome_data.xlsx")
```

### Grouping process

The thematic grouping methodology is applied to the separated descriptions of **mistakes**, **causes** and **outcomes** separately.

#### Preparing data for creating the codes

First, we read the output of the separation process of the mistakes, causes and outcomes.

Objects with the name "processed_[mistake, cause, outcome]_data" contain the result of the manually carried out response separation process.

Second, we transform the data to prepare them for the coding of the separated responses.

Third, we save the transformed datatable.

##### Read data

```{r}
processed_mistake_data <- read_xlsx("Data/Processed/grouping/separation/ReMis_Main_Processed_Mistake_data.xlsx")

processed_cause_data <- read_xlsx("Data/Processed/grouping/separation/ReMis_Main_Processed_Cause_data.xlsx")

processed_outcome_data <- read_xlsx("Data/Processed/grouping/separation/ReMis_Main_Processed_Outcome_data.xlsx")
```

##### Transform data

```{r}
processed_mistake_data <-
  processed_mistake_data %>% 
  gather(key = "mistake_no", value = "mistake_sep", -response_id, -mistake_type, -mistake) %>%
  mutate(mistake_no = str_extract(mistake_no, "[0-9]$"),
         mistake_no = as.integer(mistake_no),
         code = NA_character_) %>% 
  drop_na(mistake_sep)

processed_cause_data <-
  processed_cause_data %>% 
  gather(key = "cause_no", value = "cause_sep", -response_id, -cause_type, -cause) %>%
  mutate(cause_no = str_extract(cause_no, "[0-9]$"),
         cause_no = as.integer(cause_no),
         code = NA_character_) %>% 
  drop_na(cause_sep)

processed_outcome_data <- 
  processed_outcome_data %>% 
  gather(key = "outcome_no", value = "outcome_sep", -response_id, -outcome_type, -outcome) %>%
  mutate(outcome_no = str_extract(outcome_no, "[0-9]$"),
         outcome_no = as.integer(outcome_no),
         code = NA_character_) %>% 
  drop_na(outcome_sep)
```

##### Write data

```{r}
# write_xlsx(processed_mistake_data, "Data/Processed/grouping/coding/ReMis_Main_Processed_Mistake_Type_Coding_data.xlsx")

# write_xlsx(processed_cause_data, "Data/Processed/grouping/coding/ReMis_Main_Processed_Cause_Coding_data.xlsx")

# write_xlsx(processed_outcome_data, "Data/Processed/grouping/coding/ReMis_Main_Processed_Outcome_Coding_data.xlsx")
```

#### Creating groups

First, we read the output of the coding process of the separated mistakes, causes and outcomes.

Objects with the name "processed_[mistake_tpye, cause, outcome]_coding" contain the result of the manually carried out creating codes process.

Second, we transform the codes to use them for creating the groups.

Third, we save the transformed datatable.

##### Read data

```{r}
processed_mistake_type_coding <-
  read_xlsx("Data/Processed/grouping/coding/ReMis_Main_Processed_Mistake_Type_Coding_data.xlsx") %>% 
  mutate(mistake_no = as.integer(mistake_no))

processed_cause_coding <-
  read_xlsx("Data/Processed/grouping/coding/ReMis_Main_Processed_Cause_Coding_data.xlsx") %>% 
  mutate(cause_no = as.integer(cause_no))

processed_outcome_coding <-
  read_xlsx("Data/Processed/grouping/coding/ReMis_Main_Processed_Outcome_Coding_data.xlsx") %>% 
  mutate(outcome_no = as.integer(outcome_no))
```

##### Pre grouping exclusion

After the response separation we excluded cases if: (1) the participant’s response was irrelevant to the question, ambivalent ("see above"), or missing; or (2) stated that the mistake occurred before the prescribed timeframe (i.e., past 5 years).

##### Number of exluded responses

```{r}
keep_excluded_ambiguous_responses <- function(df, type_var, var) {
  df %>% 
    filter(code %in% c("see above",
                       "missing",
                       "irrelevant content",
                       "out of timeframe")) %>% 
    filter(response_id %ni% missing_background) %>% 
    mutate(exclusion_criteria = code,
           type = {{type_var}}) %>%
    select(response_id, type, {{var}}, exclusion_criteria) %>% 
      # In two cases for the mistake types the response got separated into 2 responses but both got the same exclusion code. In order to keep the numbers constant we keep only one response. For the cause and outcome there were no separated responses that got excluded here.
    distinct(response_id, type, .keep_all = TRUE)
}

exclude_pregrouping_mistake <- 
  processed_mistake_type_coding %>% 
  keep_excluded_ambiguous_responses(mistake_type, mistake)

exclude_pregrouping_cause <- 
  processed_cause_coding %>% 
  keep_excluded_ambiguous_responses(cause_type, cause)

exclude_pregrouping_outcome <- 
  processed_outcome_coding %>% 
  keep_excluded_ambiguous_responses(outcome_type, outcome)
```

Number of responses that were excluded before grouping. We flagged these mistakes while creating the codes to save time, therefore, the table can contain more response from one participant hence the coding was after separation. To aid comprehension in the manuscript we report the number of excluded trials at this stage before the separation. Here I only keep one response per mistake type as even if a response got separated both responses should have the same reason for exclusion and should be excluded (one of the following: see above, missing, irrelevant content, out of timeframe).

```{r}
n_response_excluded_ambiguous <- function(df) {
  df %>% 
    count(exclusion_criteria, type) %>% 
    group_by(type) %>% 
    mutate(sum = sum(n),
           exclusion_criteria = 
             case_when(
               exclusion_criteria == "see above" ~ "ambivalent", 
               TRUE ~ exclusion_criteria))
}

n_response_excluded_ambiguous(exclude_pregrouping_mistake)

n_response_excluded_ambiguous(exclude_pregrouping_cause)

n_response_excluded_ambiguous(exclude_pregrouping_outcome)
```

Numbers without breaking down at each criteria.

```{r}
count(exclude_pregrouping_mistake, type)

count(exclude_pregrouping_cause, type)

count(exclude_pregrouping_outcome, type)
```

##### Number of responses after exclusion

```{r}
n_response_left_after_excluding_ambiguous <- function(df, type_var) {
  df %>% 
    filter(code %ni% c("see above",
                       "missing",
                       "irrelevant content",
                       "out of timeframe")) %>%
    filter(response_id %ni% missing_background) %>% 
    distinct(response_id, {{type_var}}) %>% 
    count({{type_var}}) %>% 
    mutate(N = sum(n))
}

processed_mistake_type_coding %>% 
  n_response_left_after_excluding_ambiguous(., mistake_type)

processed_cause_coding %>% 
  n_response_left_after_excluding_ambiguous(., cause_type)
  
processed_outcome_coding %>% 
  n_response_left_after_excluding_ambiguous(., outcome_type)
```

##### Number of responses after separation

```{r}
n_response_after_separation <-function(df, type_var) {
  df %>% 
    filter(code %ni% c("see above",
                       "missing",
                       "irrelevant content",
                       "out of timeframe")) %>%
    filter(response_id %ni% missing_background) %>% 
    count({{type_var}}) %>% 
    mutate(N = sum(n))
}

processed_mistake_type_coding %>% 
  n_response_after_separation(., mistake_type)

processed_cause_coding %>% 
  n_response_after_separation(., cause_type)
  
processed_outcome_coding %>%
  n_response_after_separation(., outcome_type)
```

#### Grouping exclusion
##### Number of responses excluded because of insufficient information

For the thematic analysis only we exclude responses where there is not enough information in the response to decide what was the mistake ("insufficient information"). This exclusion happened based on the assigned codes.

```{r}
processed_mistake_type_coding %>% 
  filter(code == "insufficient information") %>% 
  count()

processed_cause_coding %>% 
  filter(code == "insufficient information") %>% 
  count()

processed_outcome_coding %>% 
  filter(code == "insufficient information") %>% 
  count()
```

##### Number of responses after exclusion

After excluding the responses that could not be assigned to a code.

```{r}
n_response_left_excluded_during_coding <- function(df, type_var) {
  df %>% 
    filter(code %ni% c("see above",
                       "missing",
                       "irrelevant content",
                       "out of timeframe",
                       "insufficient information")) %>% 
    filter(response_id %ni% missing_background) %>% 
    count({{type_var}}) %>% 
    mutate(N = sum(n))
}

processed_mistake_type_coding %>% 
  n_response_left_excluded_during_coding(., mistake_type)

processed_cause_coding %>% 
  n_response_left_excluded_during_coding(., cause_type)

processed_outcome_coding %>% 
  n_response_left_excluded_during_coding(., outcome_type)
```

#### Transform data

```{r}
processed_mistake_type_coding_count <-
  processed_mistake_type_coding %>% 
  group_by(code) %>%
  count() %>% 
  mutate(group = NA_character_) %>%
  arrange(code)

processed_cause_coding_count <-
  processed_cause_coding %>% 
  group_by(code) %>%
  count() %>%
  mutate(group = NA_character_) %>% 
  arrange(code)

processed_outcome_coding_count <-
  processed_outcome_coding %>% 
  group_by(code) %>%
  count() %>%
  mutate(group = NA_character_) %>% 
  arrange(code)
```

#### Counting the number of disctinct codes

```{r}
n_distinct_code <- function(df) {
  df %>% 
    filter(code %ni% c("see above",
                       "missing",
                       "irrelevant content",
                       "out of timeframe",
                       "insufficient information")) %>% 
    ungroup() %>% 
    summarise(n = n())
}

n_distinct_code(processed_mistake_type_coding_count)

n_distinct_code(processed_cause_coding_count)

n_distinct_code(processed_outcome_coding_count)
```

#### Write data

```{r}
# write_xlsx(processed_mistake_type_coding_count, "Data/Processed/grouping/group/ReMis_Main_Processed_Mistake_Type_Grouping.xlsx")

# write_xlsx(processed_cause_coding_count, "Data/Processed/grouping/group/ReMis_Main_Processed_Cause_Grouping.xlsx")

# write_xlsx(processed_outcome_coding_count, "Data/Processed/grouping/group/ReMis_Main_Processed_Outcome_Grouping.xlsx")
```

#### Testing similarity between tables

The tables are handled in a spreadsheet therefore, it is possible that there are dissimilarities between them. These test compare the parts of the tables that should be similar after the manual processing that were carried out outside of R.

```{r}
setdiff(
  select(processed_mistake_type_coding, -code),
  select(processed_mistake_data, -code)
  )

setdiff(
  select(processed_cause_coding, -code),
  select(processed_cause_data, -code)
  )

setdiff(
  select(processed_outcome_coding, -code),
  select(processed_outcome_data, -code)
  )
```

### Exclusion

First, we read the outcome of the creating groups process.
Second, we join the outputs of the creating codes and the defining groups processes.

Third, we exclude all the separated responses, where
* the corresponding code was not assigned to a group during the defining groups process ("NA")
* the respondent did not write down the response but referred to a previous response ("see above")
* the response was irrelevant to the given question ("irrelevant content")
* the response did not provide enough information to decide what did the respondent mean ("insufficient information")
* if the respondent explicitly claimed the the mistake happen more then 5 years ago ("out of timeframe")
* if the response was missing ("missing")

#### Read data

```{r}
processed_mistake_type_grouping <- read_xlsx("Data/Processed/grouping/group/ReMis_Main_Processed_Mistake_Type_Grouping.xlsx")

processed_cause_grouping <- read_xlsx("Data/Processed/grouping/group/ReMis_Main_Processed_Cause_Grouping.xlsx")

processed_outcome_grouping <- read_xlsx("Data/Processed/grouping/group/ReMis_Main_Processed_Outcome_Grouping.xlsx")
```

#### Number of distinct groups

```{r}
n_distinct_groups <- function(df) {
  df %>% 
    distinct(group) %>% 
    filter(group %ni% c("see above",
                        "missing",
                        "irrelevant content",
                        "out of timeframe",
                        "insufficient information",
                        "NA")) %>%
    count()
}

n_distinct_groups(processed_mistake_type_grouping)

n_distinct_groups(processed_cause_grouping)

n_distinct_groups(processed_outcome_grouping)
```

#### Merge each case with their group based on the codes

```{r}
processed_mistake_type_grouped <-
  processed_mistake_type_coding %>% 
  left_join(., processed_mistake_type_grouping, by = "code")

processed_cause_grouped <-
  processed_cause_coding %>% 
  left_join(., processed_cause_grouping, by = "code")

processed_outcome_grouped <- 
  processed_outcome_coding %>% 
  left_join(., processed_outcome_grouping, by = "code")
```

#### Write responses for validation

```{r}
# write_tsv(processed_mistake_type_grouped, "Data/Processed/grouping/grouped/ReMis_Main_Processed_Mistake_Type_Grouped_data.tsv")

# write_tsv(processed_cause_grouped, "Data/Processed/grouping/grouped/ReMis_Main_Processed_Cause_Grouped_data.tsv")

# write_tsv(processed_outcome_grouped, "Data/Processed/grouping/grouped/ReMis_Main_Processed_Outcome_Grouped_data.tsv")
```

#### Count the number of responses that we exclude because we could not assign the code to a group

```{r}
processed_mistake_type_grouped %>% 
  filter(group == "NA") %>%
  summarise(n = n())

processed_cause_grouped %>% 
  filter(group == "NA") %>% 
  summarise(n = n())

processed_outcome_grouped %>% 
  filter(group == "NA") %>% 
  summarise(n = n())
```

#### The number of responses after we exclude the responses that could not be assigned to a group

```{r}
n_response_left_excluded_during_grouping <- function(df, type_var) {
  df %>% 
    filter(group %ni% c("see above",
                        "missing",
                        "irrelevant content",
                        "out of timeframe",
                        "insufficient information",
                        "NA")) %>%
    filter(response_id %ni% missing_background) %>% 
    group_by({{type_var}}) %>% 
    summarise(n = n()) %>% 
    ungroup() %>% 
    mutate(N = sum(n))
}

processed_mistake_type_grouped %>% 
  n_response_left_excluded_during_grouping(., mistake_type)

processed_cause_grouped %>% 
  n_response_left_excluded_during_grouping(., cause_type)

processed_outcome_grouped %>% 
  n_response_left_excluded_during_grouping(., outcome_type)
```

#### Test if there is any case thats code is not grouped

```{r}
processed_mistake_type_grouped %>% 
  filter(is.na(group)) %>% 
  summarise(n = n())

processed_cause_grouped %>% 
  filter(is.na(group)) %>% 
  summarise(n = n())

processed_outcome_grouped %>% 
  filter(is.na(group)) %>% 
  summarise(n = n())
```

#### Create a variable that stores whether the maximum number of separated responses provided by the respondent is one or not

```{r}
processed_mistake_type_grouped <-
  processed_mistake_type_grouped %>% 
  group_by(response_id, mistake_type) %>%
  mutate(maximum_response_one_type = case_when(which.max(mistake_no) == 1L ~ "yes",
                                          which.max(mistake_no) != 1L ~ "no"))

processed_cause_grouped <-
  processed_cause_grouped %>% 
  group_by(response_id, cause_type) %>%
  mutate(maximum_response_one_cause = case_when(which.max(cause_no) == 1L ~ "yes",
                                          which.max(cause_no) != 1L ~ "no"))

processed_outcome_grouped <-
  processed_outcome_grouped %>% 
  group_by(response_id, outcome_type) %>%
  mutate(maximum_response_one_outcome = case_when(which.max(outcome_no) == 1L ~ "yes",
                                          which.max(outcome_no) != 1L ~ "no"))
```

#### Number of responses where the maximum number of separated response is not one

```{r}
processed_mistake_type_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_type != "yes") %>% 
  summarise(n = n())

processed_cause_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_cause != "yes") %>% 
  summarise(n = n())

processed_outcome_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_outcome != "yes") %>% 
  summarise(n = n())
```

#### Number of respondent who wrote more than one response

```{r}
processed_mistake_type_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_type != "yes") %>% 
  distinct(response_id) %>% 
  summarise(n = n())

processed_cause_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_cause != "yes") %>% 
  distinct(response_id) %>% 
  summarise(n = n())

processed_outcome_grouped %>% 
  ungroup() %>% 
  filter(maximum_response_one_outcome != "yes") %>% 
  distinct(response_id) %>% 
  summarise(n = n())
```

#### Exclude cases based on ambiguous groups

```{r}
keep_responses_grouping_exclusion <- function(df) {
  df %>% 
    filter(group %ni% c("see above",
                        "missing",
                        "irrelevant content",
                        "out of timeframe",
                        "insufficient information",
                        "NA")) %>% 
    # All the responses from this person will be dropped because they are missing but I want to make this explicit
    filter(response_id %ni% missing_background)
}

processed_mistake_type_filtered <-
  keep_responses_grouping_exclusion(processed_mistake_type_grouped) 

processed_cause_filtered <-
  keep_responses_grouping_exclusion(processed_cause_grouped)

processed_outcome_filtered <-
  keep_responses_grouping_exclusion(processed_outcome_grouped) 
```

#### Transformation

```{r}
processed_mistake_type_filtered <-
  processed_mistake_type_filtered %>% 
  ungroup() %>% 
  mutate(type = mistake_type,
         group_type = group,
         code_type = code) %>% 
  select(-mistake_type, -n, -group, -code)

processed_cause_filtered <-
  processed_cause_filtered %>% 
  ungroup() %>% 
  mutate(type = cause_type,
         group_cause = group,
         code_cause = code) %>% 
  select(-cause_type, -n, -group, -code)

processed_outcome_filtered <-
  processed_outcome_filtered %>% 
  ungroup() %>% 
  mutate(type = outcome_type,
         group_outcome = group,
         code_outcome = code) %>% 
  select(-outcome_type, -n, -group, -code)
```

#### Write final groups for further analysis

```{r}
write_tsv(processed_mistake_type_filtered, "Data/Processed/ReMis_Main_Processed_Type_Groups_data.tsv")

write_tsv(processed_cause_filtered, "Data/Processed/ReMis_Main_Processed_Cause_Groups_data.tsv")

write_tsv(processed_outcome_filtered, "Data/Processed/ReMis_Main_Processed_Outcome_Groups_data.tsv")
```

### Create datatable for exploring the relationship between mistake causes and types

We keep every case where the respondent reported only maximum one cause or one mistake.

#### Counting the number of causes before exclusion 

```{r}
processed_cause_filtered %>% 
  group_by(type) %>% 
  count()
```

#### Counting the number of mistake types before exclusion

```{r}
processed_mistake_type_filtered %>% 
  group_by(type) %>% 
  count()
```

#### Create the datatable

```{r}
processed_cause_type_relationship <-
  processed_cause_filtered %>% 
  inner_join(., processed_mistake_type_filtered, by = c("response_id", "type")) %>% 
  filter(!(maximum_response_one_cause == "no" & maximum_response_one_type == "no"))
```

#### Write processed datafile for analysis

```{r}
write_tsv(processed_cause_type_relationship, "Data/Processed/ReMis_Main_Processed_Cause_Type_Relationship_Analysis_data.tsv")
```

### Create datatable for exploring the relationship between mistake types and outcomes

We keep every case where the respondent reported only maximum one outcome or one mistake.

#### Counting the number of outcomes before exclusion 

```{r}
processed_outcome_filtered %>% 
  group_by(type) %>% 
  count()
```

#### Create the datatable

```{r}
processed_outcome_type_relationship <-
  processed_mistake_type_filtered %>% 
  inner_join(., processed_outcome_filtered, by = c("response_id", "type")) %>% 
  filter(!(maximum_response_one_outcome == "no" & maximum_response_one_type == "no"))
```

#### Write processed datafile for analysis

```{r}
write_tsv(processed_outcome_type_relationship, "Data/Processed/ReMis_Main_Processed_Outcome_Type_Relationship_Analysis_data.tsv")
```

## Preprocessing of the frequency and seriousness ratings
### Create datatable for the descriptive information
#### Select needed variables

```{r}
processed_descriptives <- 
  raw %>%
  select(response_id,
         years,
         field)
```

#### Write processed datafile for analysis

```{r}
write_tsv(processed_descriptives, "Data/Processed/ReMis_Main_Processed_Descriptive_data.tsv")
```

### Create datatable for general frequency of mistakes in teams
#### Select needed variables

```{r}
processed_general_frequency <- 
  raw %>%
  select(response_id,
         general_frequency)
```

#### Drop missing responses

The number of missing response for the general frequency of mistakes rating

```{r}
processed_general_frequency %>% 
  filter(is.na(general_frequency)) %>% 
  count()

processed_general_frequency <- 
  processed_general_frequency %>% 
  filter(!is.na(general_frequency))
```

There are `r nrow(processed_general_frequency)` responses left after excluding the missing responses.

#### Write processed datafile for analysis

```{r}
write_tsv(processed_general_frequency, "Data/Processed/ReMis_Main_Processed_General_Frequency_data.tsv")
```

## Create datatable for general overview of data management mistakes (seriousness and frequency ratings of mistakes)
### Investigating the excluded responses
#### Create a table of the excluded responses

We flagged the description of the mistakes that needed to be excluded from the analysis of the frequency and seriousness ratings during the preprocessing of the free-text responses in order to save time. Therefore, we use the coded mistake dataset to excluded cases that fit our exclusion criteria.

In the manuscript these are the first (missing responses) and second step (irrelevant, out of timeframe, and ambiguous responses) of the exclusions for the Preprocessing of the frequency and seriousness ratings.

```{r}
processed_mistake_type_coding <-
  read_xlsx("Data/Processed/grouping/coding/ReMis_Main_Processed_Mistake_Type_Coding_data.xlsx") %>% 
  mutate(mistake_no = as.integer(mistake_no))

exclude <-  
  processed_mistake_type_coding %>%
  filter(code %in% c("see above",
                     "missing",
                     "irrelevant content",
                     "out of timeframe")) %>%
  # As the respondent with missing background information was decided to be excluded later in the process we have to exclude them now here
  filter(response_id %ni% missing_background) %>%
  mutate(exclusion_criteria = code,
         type = mistake_type) %>%
  select(response_id, type, mistake, exclusion_criteria) %>% 
  # In case of two respondents the description of the mistake got separated into 2 responses but both got the same code. In order to keep the numbers constant we keep only one response.
  distinct(response_id, type, .keep_all = TRUE)

# write_tsv(exclude, "Data/Processed/ReMis_Main_Mistake_General_Exclusion_data.tsv")
```

#### Number of mistake responses that we exclude (and the reason why)

```{r}
exclude_table <- 
  exclude %>% 
  count(exclusion_criteria, type) %>% 
  group_by(exclusion_criteria) %>% 
  mutate(sum = sum(n),
         exclusion_criteria = case_when(exclusion_criteria == "see above" ~ "ambivalent", 
                                        TRUE ~ exclusion_criteria)) %>% 
  rename(`Exclusion criteria` = exclusion_criteria,
         `Mistake type` = type,
         `Number of mistakes excluded for the mistake type` = n,
         `Number of mistakes excluded because of the criteria` = sum) %>% 
  arrange(match(`Exclusion criteria`, c("missing", "ambivalent", "irrelevant content", "out of timeframe")))

papaja::apa_table(
  exclude_table,
  caption = "The Number of Mistakes Excluded Because of Each Criteria",
  escape = TRUE
)
```

### Transforming data
#### Creating temporary datatable for storing mistake descriptions

```{r}
temp_story <- 
  raw %>% 
  select(response_id, recurring_story, serious_story) %>%
  gather(key = "type", value = "mistake", -response_id) %>% 
  mutate(type = str_extract(type, "[^_]+"))
```

#### Number of all mistake response stories after excluding the missing responses

```{r}
temp_story %>% 
  filter(!is.na(mistake)) %>%
  count(type)
```

#### Excluding trials

```{r}
temp_story_excluded <-
  temp_story %>% 
  anti_join(., exclude, by = c("response_id", "type", "mistake"))
```

#### Number of mistake description after exclusiong by type

```{r}
temp_story_excluded %>% 
  count(type)
```

#### Creating temporary datatable for storing mistake ratings

```{r}
temp_rating <-
  raw %>%
  select(response_id, recurring_frequency, recurring_serious, serious_serious) %>% 
  gather(key = "key", value = "rating", -response_id) %>% 
  separate(key, into = c("type", "question"), sep = "_")
```

#### Number of missing mistake response ratings per type and per question before

This is the number of missing ratings before we exclude the descriptions that are missing or ambiguous.

```{r}
temp_rating %>%
  filter(is.na(rating)) %>% 
  count(type, question)
```

#### Number of all mistake response ratings after excluding the missing responses

This is the number of non-missing ratings before we exclude the descriptions that are missing or ambiguous.

```{r}
temp_rating %>%
  filter(!is.na(rating)) %>%
  count(type, question)
```

#### Joining ratings to the descriptions of the mistakes

```{r}
processed_mistake_general <-
  temp_story_excluded %>% 
  inner_join(., temp_rating, by = c("response_id", "type"))
```

#### Number of missing ratings after exclusion

```{r}
processed_mistake_general %>% 
  filter(is.na(rating)) %>% 
  count(type, question)
```

#### Drop responses where the rating is missing

```{r}
processed_mistake_general_excluded <-
  processed_mistake_general %>% 
  filter(!is.na(rating))
```

#### Number of all mistake response ratings

```{r}
processed_mistake_general_excluded %>% 
  count(type, question)
```

#### Number of individual respondents left after exclusion

```{r}
processed_mistake_general_excluded %>% 
  distinct(response_id) %>%
  count()
```

#### Write processed datafile for analysis

```{r}
write_tsv(processed_mistake_general_excluded, "Data/Processed/ReMis_Main_Processed_Mistake_General_data.tsv")
```